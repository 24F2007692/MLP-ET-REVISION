<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MLP Handwritten Revision Notes</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Kalam:wght@300;400;700&display=swap" rel="stylesheet">
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']]
            }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        body {
            background-color: #e0e0e0;
            font-family: 'Kalam', cursive;
            color: #1a237e; /* Blue Ink */
            margin: 0;
            padding: 20px;
        }

        .notebook-paper {
            background-color: #fff;
            background-image: linear-gradient(#e5e7eb 1px, transparent 1px);
            background-size: 100% 2rem;
            width: 100%;
            max-width: 900px;
            margin: 0 auto;
            padding: 3rem 4rem 3rem 5rem;
            position: relative;
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
            line-height: 2rem;
            font-size: 1.25rem;
        }

        /* Red Margin Line */
        .notebook-paper::before {
            content: '';
            position: absolute;
            top: 0;
            bottom: 0;
            left: 3.5rem;
            width: 2px;
            background-color: #ef4444; /* Red margin */
        }

        h1 {
            font-size: 2.5rem;
            font-weight: 700;
            text-align: center;
            text-decoration: underline;
            text-decoration-color: #b71c1c;
            margin-bottom: 2rem;
            line-height: 3rem;
            color: #000;
        }

        h2 {
            font-size: 1.75rem;
            font-weight: 700;
            margin-top: 2rem;
            margin-bottom: 1rem;
            border-bottom: 2px solid #1a237e;
            display: inline-block;
            line-height: 2.5rem;
        }

        h3 {
            font-size: 1.4rem;
            font-weight: 700;
            margin-top: 1rem;
            text-decoration: underline;
            line-height: 2rem;
        }

        p, li {
            margin-bottom: 0; /* Align with lines */
        }

        ul {
            list-style-type: none;
            padding-left: 0;
        }

        li::before {
            content: '-';
            margin-right: 0.5rem;
            font-weight: bold;
            color: #000;
        }

        .highlight {
            background-color: rgba(255, 255, 0, 0.3); /* Highlighter effect */
            padding: 0 4px;
            border-radius: 4px;
        }

        .trap {
            color: #b71c1c; /* Red Ink for Traps */
            font-weight: bold;
        }

        .math-box {
            background-color: rgba(255, 255, 255, 0.8);
            border: 2px dashed #1a237e;
            padding: 1rem;
            margin: 1rem 0;
            text-align: center;
        }

        .code-snippet {
            font-family: 'Courier New', Courier, monospace;
            background-color: #f3f4f6;
            padding: 0 4px;
            border-radius: 4px;
            font-size: 1rem;
            font-weight: bold;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1rem 0;
            font-size: 1.1rem;
        }
        
        th {
            border-bottom: 2px solid #000;
            text-align: left;
            padding: 0.5rem;
        }
        .watermark {
            text-align: right;
            margin-top: 50px;
            font-size: 2rem;
            color: rgba(44, 62, 80, 0.5);
            font-family: 'Kalam', cursive;
            transform: rotate(-2deg);
        }
        
        td {
            border-bottom: 1px solid #ccc;
            padding: 0.5rem;
        }

        @media print {
            body {
                background-color: white;
            }
            .notebook-paper {
                box-shadow: none;
                padding: 0;
                margin: 0;
                width: 100%;
            }
        }
    </style>
</head>
<body>

    <div class="notebook-paper">
        <h1>MLP Ultimate Revision Guide</h1>
        <p style="text-align: center; color: #b71c1c; font-weight: bold;">Mission 100/100 | Focus on TRAPS</p>

        <!-- Section 1 -->
        <h2>Section 1: The Golden Rules</h2>
        <ul>
            <li><strong>Pipeline Order:</strong> Always <span class="highlight">Impute &rarr; Scale &rarr; Model</span>.</li>
            <li><strong>Data Leakage:</strong> NEVER call <span class="code-snippet">fit()</span> on Test Data. Only <span class="code-snippet">transform()</span>.</li>
            <li><strong>Confusion Matrix:</strong> Sklearn Standard: <strong>Rows = True</strong>, <strong>Cols = Predicted</strong>.</li>
            <li><strong>Imbalanced Data:</strong> Accuracy is <span class="trap">useless</span>. Use F1-Score or AUC.</li>
            <li><strong>Scaling Requirement:</strong> SGD, KNN, SVM <span class="trap">REQUIRE</span> scaling. Trees do not.</li>
            <li><strong>Partial Fit:</strong> <span class="code-snippet">model.partial_fit(X, y)</span> allows incremental learning. You <span class="trap">MUST</span> pass classes in the first call for classification.</li>
            <li><strong>GridSearch Counting:</strong> Fits = $ \text{cv} \times (\text{combinations}) + 1 \text{ (refit)} $.</li>
        </ul>

        <!-- Section 2 -->
        <h2>Section 2: Code Maths & Shapes</h2>
        
        <h3>1. Pandas Power</h3>
        <ul>
            <li><strong>groupby:</strong> Split-apply-combine strategy. <span class="code-snippet">df.groupby('Cat')['Qty'].sum()</span></li>
            <li><strong>dropna:</strong></li>
            <li style="margin-left: 20px;"><code>how='any'</code>: Drops if ANY NA.</li>
            <li style="margin-left: 20px;"><code>how='all'</code>: Drops if ALL NA.</li>
            <li style="margin-left: 20px;"><code>subset=['col']</code>: Checks specific cols only.</li>
        </ul>

        <h3>2. Preprocessing Traps</h3>
        <ul>
            <li><strong>SimpleImputer:</strong> Strategies: 'mean', 'median', 'most_frequent'.</li>
            <li><strong>StandardScaler:</strong> Centers data ($mean=0, \sigma=1$).</li>
            <li><strong>MinMaxScaler:</strong> Scales to [0, 1]. <span class="trap">Sensitive to outliers.</span></li>
            <li><strong>MaxAbsScaler:</strong> Divides by max absolute value. <span class="highlight">Preserves sparsity.</span></li>
            <li><strong>ColumnTransformer:</strong> <span class="code-snippet">remainder='passthrough'</span> keeps columns. Default drops them!</li>
            <li><strong>OneHotEncoder:</strong> <span class="code-snippet">sparse_output=False</span> gives array. Trap: 1 col with $k$ values &rarr; $k$ new cols.</li>
        </ul>

        <h3>3. ColumnTransformer Logic</h3>
        <p>Scenario: MinMaxScaler (A) + OHE(drop='first') (B)</p>
        <ul>
            <li><strong>Order:</strong> Output order = Code order.</li>
            <li><strong>Categorical Sorting:</strong> Always sort A-Z first!</li>
            <li><strong>Drop First:</strong> The first alphabetically ('Apple') gets dropped.</li>
        </ul>
        <p class="trap">Sort karna mat bhoolna!</p>

        <h3>4. Shape Shifters</h3>
        <ul>
            <li><strong>FeatureUnion:</strong> Adds cols side-by-side.</li>
            <li><strong>Poly Features (Deg 2):</strong> Formula = $\frac{N(N+3)}{2}$</li>
            <li><strong>Image Flattening:</strong></li>
            <li style="margin-left: 20px;">RGB (200, 200, 3) &rarr; <strong>120,000</strong>.</li>
            <li style="margin-left: 20px;">Gray (200, 200) &rarr; <strong>40,000</strong> (3 Gayab!).</li>
        </ul>

        <!-- Section 3 -->
        <h2>Section 3: Metric Masterclass</h2>
        
        <h3>1. Confusion Matrix</h3>
        <p><strong>Precision:</strong> $ \frac{TP}{TP + FP} $ (Controls False Positives).</p>
        <p><strong>Recall:</strong> $ \frac{TP}{TP + FN} $ (Controls False Negatives).</p>
        <p><strong>Specificity (TNR):</strong> Correctly identified Negatives.</p>

        <div class="math-box">
            <strong>Recall Calculation Example (PYQ 1376)</strong><br>
            True Label 2 (Row 2): [3, 9, 3] &rarr; Total = 15.<br>
            Correct Predicted (at 2,2): 3.<br>
            $ \text{Recall} = \frac{3}{15} = 0.20 $
        </div>

        <h3>2. Naive Bayes Smoothing</h3>
        <p>$$ P(x|y) = \frac{\text{count}(x, y) + \alpha}{\text{count}(y) + \alpha \cdot d} $$</p>

        <h3>3. Metric Dictionary</h3>
        <ul>
            <li><strong>Macro Avg:</strong> Simple Average. Treats all classes equally.</li>
            <li><strong>Weighted Avg:</strong> Weighted by support (size). Favors Majority.</li>
            <li><strong>Accuracy:</strong> Use ONLY if balanced.</li>
            <li><strong>F1-score:</strong> Best for imbalanced data.</li>
            <li><strong>ROC-AUC:</strong> Threshold-independent. Ranks Positive > Negative.</li>
            <li><strong>PR-AUC:</strong> Precision-Recall. For HIGHLY imbalanced data.</li>
            <li><strong>Log Loss:</strong> Measures probability/confidence quality.</li>
        </ul>

        <h3>4. The R2 Score Formula</h3>
        <div class="math-box">
            $$ R^2 = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2} $$
        </div>
        <p class="trap">Trap: SS<sub>tot</sub> (Variance) always comes in Denominator.</p>

        <!-- Section 4 -->
        <h2>Section 4: Parameters (Rutta Maar Lo)</h2>
        
        <table>
            <thead>
                <tr>
                    <th>Algo</th>
                    <th>Param</th>
                    <th>Meaning</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Logistic</td>
                    <td>solver='saga'</td>
                    <td>Required for ElasticNet.</td>
                </tr>
                <tr>
                    <td>Logistic</td>
                    <td>solver='liblinear'</td>
                    <td>Good for small datasets.</td>
                </tr>
                <tr>
                    <td>Log/SVM</td>
                    <td>multi_class</td>
                    <td>'ovr' (One-vs-Rest) or 'multinomial'.</td>
                </tr>
                <tr>
                    <td>SGD(Reg)</td>
                    <td>loss</td>
                    <td>'squared_error', 'huber'.</td>
                </tr>
                <tr>
                    <td>SGD(Clf)</td>
                    <td>loss</td>
                    <td>'log_loss', 'hinge' (SVM).</td>
                </tr>
                <tr>
                    <td>SGD</td>
                    <td>lr</td>
                    <td>'constant', 'invscaling', 'adaptive'.</td>
                </tr>
                <tr>
                    <td>KNN</td>
                    <td>weights</td>
                    <td>'uniform' vs 'distance'.</td>
                </tr>
                <tr>
                    <td>Boosting</td>
                    <td>learning_rate</td>
                    <td>Shrinks contribution. Low LR = More trees.</td>
                </tr>
                <tr>
                    <td>RF</td>
                    <td>max_features</td>
                    <td>Default: sqrt(n_features).</td>
                </tr>
            </tbody>
        </table>

        <h3>Critical Logic Blocks</h3>
        <ul>
            <li><strong>Regularization:</strong>
                <ul>
                    <li>Ridge (L2): Shrinks weights.</li>
                    <li>Lasso (L1): Zero weights (Feature Selection).</li>
                    <li>ElasticNet: Combination of L1 + L2.</li>
                </ul>
            </li>
            <li><strong>SVM Logic:</strong>
                <ul>
                    <li>Small C: Wide Margin (Underfit).</li>
                    <li>Large C: Narrow Margin (Overfit).</li>
                    <li>Kernels: 'linear', 'poly', 'rbf'.</li>
                </ul>
            </li>
            <li><strong>KNN Logic:</strong>
                <ul>
                    <li>Lazy Learner: No training phase.</li>
                    <li>Small k: High Variance (Overfit).</li>
                    <li>Large k: High Bias (Underfit).</li>
                    <li>Distance: Euclidean (L2), Manhattan (L1).</li>
                </ul>
            </li>
        </ul>

        <!-- Section 5 -->
        <h2>Section 5: Special Theory</h2>
        
        <h3>1. Unsupervised Learning</h3>
        <ul>
            <li><strong>Inertia (SSE):</strong> Sum of squared distances to centroid. Minimize this.</li>
            <li><strong>Elbow Method:</strong> Pick K where Inertia drop slows down.</li>
            <li><strong>K-Means:</strong> <span class="code-snippet">k-means++</span> speeds up but <span class="trap">NO</span> global optimum guarantee.</li>
            <li><strong>PCA:</strong> <span class="code-snippet">explained_variance_ratio_</span> shows info per component.</li>
        </ul>

        <h3>2. Time Series Analysis</h3>
        <ul>
            <li>Components: Trend, Seasonality, Noise.</li>
            <li>Additive: $T + S + E$ (Constant Amplitude).</li>
            <li>Multiplicative: $T \times S \times E$ (Growing Amplitude).</li>
        </ul>
        <div class="math-box" style="text-align: left;">
            <strong>ADF Test Decision Rule:</strong><br>
            $H_0$: Unit Root present (Non-Stationary).<br>
            p <= 0.05 &rarr; Reject $H_0$ &rarr; <strong>Stationary (Good)</strong>.<br>
            p > 0.05 &rarr; Fail to reject &rarr; <strong>Non-Stationary</strong>.
        </div>

        <h3>3. Text Processing</h3>
        <ul>
            <li><strong>CountVectorizer:</strong> Frequency count.</li>
            <li><strong>TfidfVectorizer:</strong> Penalizes frequent words (like "the").</li>
            <li><strong>Vocabulary:</strong> Returns dict {word: index}.</li>
        </ul>

        <h3>4. Target Types (Trap!)</h3>
        <ul>
            <li>[0, 1, 0] &rarr; binary</li>
            <li>[0, 1, 2] &rarr; multiclass</li>
            <li>[[0, 1], [1, 0]] &rarr; multilabel-indicator</li>
        </ul>

        <!-- Section 6 -->
        <h2>Section 6: Nano-Patterns</h2>
        <ul>
            <li><strong>Pandas Slicing:</strong>
                <ul>
                    <li><span class="code-snippet">loc[0:2]</span> &rarr; 0, 1, 2 (Inclusive).</li>
                    <li><span class="code-snippet">iloc[0:2]</span> &rarr; 0, 1 (Exclusive).</li>
                    <li><span class="trap">Trap:</span> String index loc includes last element.</li>
                </ul>
            </li>
            <li><strong>GridSearch Syntax:</strong> Correct: <span class="code-snippet">svm__C</span> (Double Underscore).</li>
            <li><strong>Train vs Test:</strong> Train &rarr; fit_transform(), Test &rarr; transform() (NEVER FIT!).</li>
            <li><strong>SelectKBest:</strong> Returns array with ONLY selected columns.</li>
            <li><strong>DummyRegressor:</strong> Always predicts Mean, ignores input.</li>
        </ul>
        
        <p style="text-align: center; margin-top: 3rem; font-size: 1.5rem; color: #b71c1c;"><strong>Good Luck! 100/100</strong></p>
    </div>
<div class="watermark">~KarTiküçÅ</div>
</body>
</html>
